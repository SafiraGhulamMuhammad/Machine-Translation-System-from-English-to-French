{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import pickle\n",
    "import string\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn\n",
    "from utils_MT import get_dict,cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The word embeddings data for English and French words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full dataset for English embeddings is about 3.64 gigabytes, and the French embeddings are about 629 megabytes. To prevent from crashing, I've extracted a subset of the embeddings for the words that we'll use in this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we'll use the subset of word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_embeddings_subset = pickle.load(open(\"en_embeddings.p\", \"rb\"))\n",
    "fr_embeddings_subset = pickle.load(open(\"fr_embeddings.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the data\n",
    "* en_embeddings_subset: the key is an English word, and the vaule is a 300 dimensional array, which is the embedding for that word.\n",
    "'the': array([ 0.08007812,  0.10498047,  0.04980469,  0.0534668 , -0.06738281, ....\n",
    "* fr_embeddings_subset: the key is an French word, and the vaule is a 300 dimensional array, which is the embedding for that word.\n",
    "'la': array([-6.18250e-03, -9.43867e-04, -8.82648e-03,  3.24623e-02,..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load two dictionaries mapping the English to French words\n",
    "* A training dictionary\n",
    "* and a testing dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Learning\\DATA SCIENCE\\NLP\\NLP_Specialization\\C1\\C1W4\\utils_MT.py:55: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  en = my_file.loc[i][0]\n",
      "d:\\Learning\\DATA SCIENCE\\NLP\\NLP_Specialization\\C1\\C1W4\\utils_MT.py:56: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  fr = my_file.loc[i][1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the English to French training dictionary is 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Learning\\DATA SCIENCE\\NLP\\NLP_Specialization\\C1\\C1W4\\utils_MT.py:55: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  en = my_file.loc[i][0]\n",
      "d:\\Learning\\DATA SCIENCE\\NLP\\NLP_Specialization\\C1\\C1W4\\utils_MT.py:56: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  fr = my_file.loc[i][1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the English to French test dictionary is 5000\n"
     ]
    }
   ],
   "source": [
    "# loading the english to french dictionaries\n",
    "en_fr_train = get_dict('en-fr.train.txt')#\n",
    "print('The length of the English to French training dictionary is', len(en_fr_train))\n",
    "en_fr_test = get_dict('en-fr.test.txt')\n",
    "print('The length of the English to French test dictionary is', len(en_fr_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "en_fr_train is a dictionary where the key is the English word and the value is the French translation of that English word.\n",
    "en_fr_test is similar to en_fr_train, but is a test set. We won't look at it until we get to testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate embedding and transform matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now implement a function get_matrices, which takes the loaded data and returns matrices X and Y.\n",
    "\n",
    "Inputs:\n",
    "* en_fr : English to French dictionary\n",
    "* en_embeddings : English to embeddings dictionary\n",
    "* fr_embeddings : French to embeddings dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns:\n",
    "\n",
    "Matrix X and matrix Y, where each row in X is the word embedding for an english word, and the same row in Y is the word embedding for the French version of that English word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrices(en_fr, french_vecs, english_vecs):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        en_fr: English to French dictionary\n",
    "        french_vecs: French words to their corresponding word embeddings.\n",
    "        english_vecs: English words to their corresponding word embeddings.\n",
    "    Output: \n",
    "        X: a matrix where the columns are the English embeddings.\n",
    "        Y: a matrix where the columns correspong to the French embeddings.\n",
    "        R: the projection matrix that minimizes the F norm ||X R -Y||^2.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # X_l and Y_l are lists of the english and french word embeddings\n",
    "    X_l = list()\n",
    "    Y_l = list()\n",
    "\n",
    "    # get the english words (the keys in the dictionary) and store in a set()\n",
    "    english_set = english_vecs.keys()\n",
    "\n",
    "    # get the french words (keys in the dictionary) and store in a set()\n",
    "    french_set = french_vecs.keys()\n",
    "\n",
    "    # store the french words that are part of the english-french dictionary (these are the values of the dictionary)\n",
    "    french_words = set(en_fr.values())\n",
    "\n",
    "    # loop through all english, french word pairs in the english french dictionary\n",
    "    for en_word, fr_word in en_fr.items():\n",
    "\n",
    "        # check that the french word has an embedding and that the english word has an embedding\n",
    "        if fr_word in french_set and en_word in english_set:\n",
    "\n",
    "            # get the english embedding\n",
    "            en_vec = english_vecs[en_word]\n",
    "\n",
    "            # get the french embedding\n",
    "            fr_vec = french_vecs[fr_word]\n",
    "\n",
    "            # add the english embedding to the list\n",
    "            X_l.append(en_vec)\n",
    "\n",
    "            # add the french embedding to the list\n",
    "            Y_l.append(fr_vec)\n",
    "\n",
    "    # stack the vectors of X_l into a matrix X\n",
    "    X = np.vstack(X_l)\n",
    "\n",
    "    # stack the vectors of Y_l into a matrix Y\n",
    "    Y = np.vstack(Y_l)\n",
    "\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the training set:\n",
    "X_train, Y_train = get_matrices(\n",
    "    en_fr_train, fr_embeddings_subset, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation as linear transformation of embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing a program that translates English words to French words using word embeddings and vector space models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given dictionaries of English and French word embeddings we will create a transformation matrix R\n",
    "\n",
    "Given an English word embedding,e , we can multiply eR to get a new word embedding f.\n",
    "Both e and f are row vectors.\n",
    "we can then compute the nearest neighbors to f in the french embeddings and recommend the word that is most similar to the transformed word embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find a matrix R that minimizes the following equation.\n",
    "                            ||X  R -Y||^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Computing the loss\n",
    "* The loss function will be squared Frobenoius norm of the difference between matrix and its approximation, divided by the number of training examples.\n",
    "* Compute the approximation of Y by matrix multiplying X and R\n",
    "* Compute difference XR - Y\n",
    "* Compute the squared Frobenius norm of the difference and divide it by.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(X, Y, R):\n",
    "    '''\n",
    "    Inputs: \n",
    "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
    "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
    "        R: a matrix of dimension (n,n) - transformation matrix from English to French vector space embeddings.\n",
    "    Outputs:\n",
    "        L: a matrix of dimension (m,n) - the value of the loss function for given X, Y and R.\n",
    "    '''\n",
    "    \n",
    "    # m is the number of rows in X\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # diff is XR - Y\n",
    "    diff = np.dot(X,R)-Y\n",
    "\n",
    "    # diff_squared is the element-wise square of the difference\n",
    "    diff_squared = diff**2\n",
    "\n",
    "    # sum_diff_squared is the sum of the squared elements\n",
    "    sum_diff_squared = np.sum(diff_squared)\n",
    "\n",
    "    # loss i the sum_diff_squard divided by the number of examples (m)\n",
    "    loss = sum_diff_squared/m\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Computing the gradient of loss in respect to transform matrix R\n",
    "* Calculate the gradient of the loss with respect to transform matrix R.\n",
    "* The gradient is a matrix that encodes how much a small change in R affect the change in the loss function.\n",
    "* The gradient gives us the direction in which we should decrease R to minimize the loss.\n",
    "* m is the number of training examples (number of rows in X ).\n",
    "* The formula for the gradient of the loss function L(X,Y,R) is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, Y, R):\n",
    "    '''\n",
    "    Inputs: \n",
    "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
    "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
    "        R: a matrix of dimension (n,n) - transformation matrix from English to French vector space embeddings.\n",
    "    Outputs:\n",
    "        g: a matrix of dimension (n,n) - gradient of the loss function L for given X, Y and R.\n",
    "    '''\n",
    "    \n",
    "    # m is the number of rows in X\n",
    "    m = X.shape[0]\n",
    "\n",
    "    # gradient is X^T(XR - Y) * 2/m\n",
    "    gradient = np.dot(X.transpose(),np.dot(X,R)-Y)*(2/m)\n",
    "    \n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Finding the optimal R with gradient descent algorithm\n",
    "* Calculate gradient g of the loss with respect to the matrix R.\n",
    "* Update R with the formula:\n",
    " R := R - alpha * g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def align_embeddings(X, Y, train_steps=100, learning_rate=0.0003):\n",
    "    '''\n",
    "    Inputs:\n",
    "        X: a matrix of dimension (m,n) where the columns are the English embeddings.\n",
    "        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.\n",
    "        train_steps: positive int - describes how many steps will gradient descent algorithm do.\n",
    "        learning_rate: positive float - describes how big steps will  gradient descent algorithm do.\n",
    "    Outputs:\n",
    "        R: a matrix of dimension (n,n) - the projection matrix that minimizes the F norm ||X R -Y||^2\n",
    "    '''\n",
    "    np.random.seed(129)\n",
    "\n",
    "    # the number of columns in X is the number of dimensions for a word vector (e.g. 300)\n",
    "    # R is a square matrix with length equal to the number of dimensions in th  word embedding\n",
    "    R = np.random.rand(X.shape[1], X.shape[1])\n",
    "\n",
    "    for i in range(train_steps):\n",
    "        if i % 25 == 0:\n",
    "            print(f\"loss at iteration {i} is: {compute_loss(X, Y, R):.4f}\")\n",
    "      \n",
    "        # use the function that you defined to compute the gradient\n",
    "        gradient = compute_gradient(X,Y,R)\n",
    "\n",
    "        # update R by subtracting the learning rate times gradient\n",
    "        R -= learning_rate * gradient\n",
    "        \n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iteration 0 is: 3.7242\n",
      "loss at iteration 25 is: 3.6283\n",
      "loss at iteration 50 is: 3.5350\n",
      "loss at iteration 75 is: 3.4442\n"
     ]
    }
   ],
   "source": [
    "# Testing  implementation.\n",
    "np.random.seed(129)\n",
    "m = 10\n",
    "n = 5\n",
    "X = np.random.rand(m, n)\n",
    "Y = np.random.rand(m, n) * .1\n",
    "R = align_embeddings(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate transformation matrix R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using those the training set, find the transformation matrix R\n",
    " by calling the function align_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iteration 0 is: 963.0146\n",
      "loss at iteration 25 is: 97.8292\n",
      "loss at iteration 50 is: 26.8329\n",
      "loss at iteration 75 is: 9.7893\n",
      "loss at iteration 100 is: 4.3776\n",
      "loss at iteration 125 is: 2.3281\n",
      "loss at iteration 150 is: 1.4480\n",
      "loss at iteration 175 is: 1.0338\n",
      "loss at iteration 200 is: 0.8251\n",
      "loss at iteration 225 is: 0.7145\n",
      "loss at iteration 250 is: 0.6534\n",
      "loss at iteration 275 is: 0.6185\n",
      "loss at iteration 300 is: 0.5981\n",
      "loss at iteration 325 is: 0.5858\n",
      "loss at iteration 350 is: 0.5782\n",
      "loss at iteration 375 is: 0.5735\n"
     ]
    }
   ],
   "source": [
    "R_train = align_embeddings(X_train, Y_train, train_steps=400, learning_rate=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the translation\n",
    "### k-Nearest neighbors algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* k-NN is a method which takes a vector as input and finds the other vectors in the dataset that are closest to it.\n",
    "* The 'k' is the number of \"nearest neighbors\" to find (e.g. k=2 finds the closest two neighbors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Searching for the translation embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nearest_neighbor(v, candidates, k=1):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      - v, the vector you are going find the nearest neighbor for\n",
    "      - candidates: a set of vectors where we will find the neighbors\n",
    "      - k: top k nearest neighbors to find\n",
    "    Output:\n",
    "      - k_idx: the indices of the top k closest vectors in sorted form\n",
    "    \"\"\"\n",
    "\n",
    "    similarity_l = []\n",
    "\n",
    "    # for each candidate vector...\n",
    "    for row in candidates:\n",
    "        # get the cosine similarity\n",
    "        cos_similarity = cosine_similarity(v,row)\n",
    "\n",
    "        # append the similarity to the list\n",
    "        similarity_l.append(cos_similarity)\n",
    "        \n",
    "    # sort the similarity list and get the indices of the sorted list\n",
    "    sorted_ids = np.argsort(similarity_l)\n",
    "\n",
    "    # get the indices of the k most similar candidate vectors\n",
    "    k_idx = sorted_ids[-k:]\n",
    "    \n",
    "    return k_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9 9 9]\n",
      " [1 0 5]\n",
      " [2 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Test  implementation:\n",
    "v = np.array([1, 0, 1])\n",
    "candidates = np.array([[1, 0, 5], [-2, 5, 3], [2, 0, 1], [6, -9, 5], [9, 9, 9]])\n",
    "print(candidates[nearest_neighbor(v, candidates, 3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test translation and compute its accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vocabulary(X, Y, R):\n",
    "    '''\n",
    "    Input:\n",
    "        X: a matrix where the columns are the English embeddings.\n",
    "        Y: a matrix where the columns correspong to the French embeddings.\n",
    "        R: the transform matrix which translates word embeddings from\n",
    "        English to French word vector space.\n",
    "    Output:\n",
    "        accuracy: for the English to French capitals\n",
    "    '''\n",
    "\n",
    "    \n",
    "    pred = np.dot(X,R)\n",
    "\n",
    "    # initialize the number correct to zero\n",
    "    num_correct = 0\n",
    "\n",
    "    # loop through each row in pred (each transformed embedding)\n",
    "    for i in range(len(pred)):\n",
    "        # get the index of the nearest neighbor of pred at row 'i'; also pass in the candidates in Y\n",
    "        pred_idx = nearest_neighbor(pred[i],Y)\n",
    "\n",
    "        # if the index of the nearest neighbor equals the row of i... \\\n",
    "        if pred_idx == i:\n",
    "            # increment the number correct by 1.\n",
    "            num_correct += 1\n",
    "\n",
    "    # accuracy is the number correct divided by the number of rows in 'pred' (also number of rows in X)\n",
    "    accuracy = num_correct / len(pred)\n",
    "\n",
    "    \n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see how is your translation mechanism working on the unseen data:\n",
    "\n",
    "X_val, Y_val = get_matrices(en_fr_test, fr_embeddings_subset, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set is 0.557\n"
     ]
    }
   ],
   "source": [
    "acc = test_vocabulary(X_val, Y_val, R_train)  # this might take a minute or two\n",
    "print(f\"accuracy on test set is {acc:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
